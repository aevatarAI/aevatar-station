name: Run Aevatar Station Regression Test on Testnet

on:
  workflow_dispatch:
    inputs:
      project_name:
        description: 'Project name to use as prefix for the namespace'
        required: true
        default: 'aevatar'
      environment_ttl:
        description: 'Time to live for the environment in minutes (0 for manual cleanup)'
        required: true
        default: '240'
      client_id:
        required: true
        type: string
        description: 'Client ID for the new app'
        default: 'regressiontest'
      client_secret:
        required: true
        type: string
        description: 'Client secret for the new app'
        default: 'regressiontest'
  push:
    branches:
      - feature/regression-test

run-name: Run Aevatar Station Regression Test on Testnet on ${{ github.ref_name }} from ${{ github.sha }}

concurrency:
  group: workflow-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: 3.11
  # Setting PYTHONUNBUFFERED to 1 to ensure that Python output is sent straight to the terminal without being buffered
  PYTHONUNBUFFERED: 1
  DOTNET_INSTALL_DIR: "./.dotnet"

jobs:
  publish:
    runs-on: aismart-runner
    strategy:
      matrix:
        servicename:
          [
            Aevatar.Silo,
            Aevatar.Developer.Host
          ]
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Setup .NET
        uses: actions/setup-dotnet@v3
        with:
          dotnet-version: '9.0.x'
      - name: Cache NuGet Packages
        id: nuget-packages
        uses: actions/cache@v4
        env:
          cache-name: nuget-package-cache
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-${{ env.cache-name }}-${{ matrix.servicename }}
      - name: List NuGet Packages
        if: ${{ steps.nuget-packages.outputs.cache-hit == 'true' }}
        continue-on-error: true
        run: ls -lh ~/.nuget/packages
      - run: dotnet publish src/${{ matrix.servicename }}/${{ matrix.servicename }}.csproj -o out/${{ matrix.servicename }}
      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.servicename }}
          path: out/${{ matrix.servicename }}
          retention-days: 1

  build-and-push-image:
    needs: publish
    runs-on: aismart-runner
    strategy:
      matrix:
        servicename:
          [
            Aevatar.Silo,
            Aevatar.Developer.Host
          ]
    permissions:
      contents: read
    outputs:
      short_sha: ${{ steps.vars.outputs.short_sha }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set short git commit SHA
        id: vars
        run: |
          calculatedSha=$(git rev-parse --short ${{ github.sha }})
          echo "short_sha=$calculatedSha" >> "$GITHUB_OUTPUT"
      - name: Download a single artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ matrix.servicename }}
          path: out/${{ matrix.servicename }}
      - name: Create image tag
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ secrets.REPOSITORY_REGION }}-docker.pkg.dev/${{ secrets.PROJECT_ID }}/${{ secrets.REPOSITORY }}/${{ matrix.servicename }}
          tags: |
            type=sha
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          build-args: |
            servicename=${{ matrix.servicename }}
          platforms: linux/amd64
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

  setup-ephermeral-test-env:
    runs-on: ubuntu-latest
    needs: build-and-push-image
    outputs:
      auth_server_url: ${{ steps.setup_env.outputs.workflow.outputs.auth_server_url }}
      api_server_url: ${{ steps.setup_env.outputs.workflow.outputs.api_server_url }} 
      app_url: ${{ steps.setup_env.outputs.workflow.outputs.app_url }}
    steps:
      - name: Setup Ephermeral Env
        id: setup_env
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.TOK }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: 'AElfDevops',
              repo: 'ephemeral-test-env',
              workflow_id: 'ephemeral-env-main.yaml',
              ref: 'main',
              inputs: {
                project_name: "regressiontest",
                environment_ttl: "1",
                client_id: "regressiontest",
                client_secret: "regressiontest",
                host_silo_image_tag: "sha-${{ needs.build-and-push-image.outputs.short_sha }}",
                host_client_image_tag: "sha-${{ needs.build-and-push-image.outputs.short_sha }}"
              }
            });
            
      - name: Wait for workflow run to complete
        id: setup_wait
        env:
          GH_TOKEN: ${{ secrets.TOK }}
        run: |
          owner="AElfDevops"
          repo="ephemeral-test-env"
          workflow_id=${{ steps.setup_env.outputs.workflow_id }}

          echo "Waiting for workflow $workflow_id to start..."
          run_id=""

          # Wait until the workflow run appears
          for i in {1..30}; do
            run_id=$(gh api repos/$owner/$repo/actions/workflows/$workflow_id/runs \
              --jq '.workflow_runs[0].id')
            if [ -n "$run_id" ]; then
              echo "Run ID: $run_id"
              break
            fi
            echo "Workflow not yet started, retrying in 10s..."
            sleep 10
          done

          # Poll the run status
          echo "Waiting for workflow run to complete..."
          for i in {1..60}; do
            status=$(gh api repos/$owner/$repo/actions/runs/$run_id --jq '.status')
            conclusion=$(gh api repos/$owner/$repo/actions/runs/$run_id --jq '.conclusion')

            echo "Status: $status | Conclusion: $conclusion"
            if [ "$status" == "completed" ]; then
              if [ "$conclusion" != "success" ]; then
                echo "Workflow failed or was cancelled"
                exit 1
              fi
              break
            fi
            sleep 10
          done

          echo "Workflow completed successfully"

      - name: Download artifact from Devops repo
        uses: dawidd6/action-download-artifact@v2
        with:
          workflow: ephemeral-env-main.yaml
          repo: AElfDevops/ephemeral-test-env
          name: setup-aevatar-app-env-output  
          github_token: ${{ secrets.TOK }}
          path: ./downloaded

      - name: Extract env data from JSON
        id: extract
        run: |
          content=$(cat ./downloaded/setup-aevatar-app-env-output.json)
          echo "auth_server_url=$(echo $content | jq -r '.auth_server_url')" >> $GITHUB_OUTPUT
          echo "api_server_url=$(echo $content | jq -r '.api_server_url')" >> $GITHUB_OUTPUT
          echo "app_url=$(echo $content | jq -r '.app_url')" >> $GITHUB_OUTPUT

  run-regression-test:
    needs: setup-ephermeral-test-env 
    runs-on: ubuntu-latest
    env:
      CLIENT_ID: ${{ secrets.TEST_CLIENT_ID }}
      CLIENT_SECRET: ${{ secrets.TEST_CLIENT_SECRET }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.arch }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.arch }}-pip-

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Python Script
        run: |
          source venv/bin/activate
          pytest -s -v regression_test.py
          pytest -s -v regression_test_signalr.py

      - name: Lark Notification on Success
        if: success()
        uses: drayeasy/action-lark-notify@main
        env:
          LARK_WEBHOOK: ${{ secrets.LARK_WEBHOOK }}
          LARK_MESSAGE_TITLE: "测试成功"

      - name: Lark Notification on Failure
        if: failure()
        uses: drayeasy/action-lark-notify@main
        env:
          LARK_WEBHOOK: ${{ secrets.LARK_WEBHOOK }}
          LARK_MESSAGE_TITLE: "测试失败"
          LARK_MESSAGE_TEMPLATE: "red"