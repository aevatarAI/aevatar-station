name: Run Aevatar Station Regression Test on Ephemeral Test Environment

on:
  workflow_dispatch:
  pull_request_review:
    types: [submitted]

run-name: Run Aevatar Station Regression Test on Ephemeral Test Environment on ${{ github.ref_name }} from ${{ github.sha }}

# concurrency:
#   group: workflow-${{ github.ref }}
#   cancel-in-progress: true

env:
  PYTHON_VERSION: 3.11
  # Setting PYTHONUNBUFFERED to 1 to ensure that Python output is sent straight to the terminal without being buffered
  PYTHONUNBUFFERED: 1
  DOTNET_INSTALL_DIR: "./.dotnet"

jobs:
  approval-count:
    if: github.event.review.state == 'approved' || github.event_name == 'workflow_dispatch'
    runs-on: ephemeral-env-runner
    outputs:
      approved_enough: ${{ steps.check.outputs.approved_enough }}
    steps:
      - name: Install GitHub CLI
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "Skip Install GitHub CLI"
            exit 0
          fi
          curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
          sudo apt update
          sudo apt install gh -y
      - name: Check Approve Count
        id: check
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "Skip approval count check"
            exit 0
          fi
          PR_NUMBER=${{ github.event.pull_request.number }}
          REPO=${{ github.repository }}
          OWNER=$(echo "$REPO" | cut -d'/' -f1)
          REPO_NAME=$(echo "$REPO" | cut -d'/' -f2)
          gh pr view $PR_NUMBER -R $REPO --json reviews > reviews.json
          jq '[.reviews | group_by(.author.login) | map(.[-1]) | map(select(.state=="APPROVED")) | .[].author.login] | unique' reviews.json > approved_users.json
          APPROVED_COUNT=0
          for user in $(jq -r '.[]' approved_users.json); do
            PERM=$(gh api repos/$OWNER/$REPO_NAME/collaborators/$user/permission --jq '.permission')
            if [[ "$PERM" == "admin" || "$PERM" == "write" ]]; then
              APPROVED_COUNT=$((APPROVED_COUNT+1))
            fi
          done
          echo "Approve count: $APPROVED_COUNT"
          if [ "$APPROVED_COUNT" -eq 1 ]; then
            echo "approved_enough=true" >> $GITHUB_OUTPUT
          else
            echo "approved_enough=false" >> $GITHUB_OUTPUT
          fi

  publish:
    if: needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch'
    needs: approval-count
    runs-on: ephemeral-env-runner
    strategy:
      matrix:
        servicename:
          [
            Aevatar.Silo,
            Aevatar.Developer.Host,
            Aevatar.HttpApi.Host,
            Aevatar.AuthServer,
            Aevatar.DbMigrator,
          ]
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Setup .NET
        uses: actions/setup-dotnet@v3
        with:
          dotnet-version: '9.0.x'
      - name: Cache NuGet Packages
        id: nuget-packages
        uses: actions/cache@v4
        env:
          cache-name: nuget-package-cache
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-${{ env.cache-name }}-${{ matrix.servicename }}
      - name: List NuGet Packages
        if: ${{ steps.nuget-packages.outputs.cache-hit == 'true' }}
        continue-on-error: true
        run: ls -lh ~/.nuget/packages
      - run: dotnet publish station/src/${{ matrix.servicename }}/${{ matrix.servicename }}.csproj -o out/${{ matrix.servicename }}
      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.servicename }}
          path: out/${{ matrix.servicename }}
          retention-days: 1

  publish-benchmarks:
    if: needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch'
    needs: approval-count
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Setup .NET
        uses: actions/setup-dotnet@v3
        with:
          dotnet-version: '9.0.x'
      - name: Cache NuGet Packages
        id: nuget-packages
        uses: actions/cache@v4
        env:
          cache-name: nuget-package-cache
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-${{ env.cache-name }}-benchmarks
      - name: Publish BroadcastLatencyBenchmark
        run: dotnet publish station/benchmark/BroadcastLatencyBenchmark/BroadcastLatencyBenchmark.csproj -o out/BroadcastLatencyBenchmark
      - name: Publish LatencyBenchmark  
        run: dotnet publish station/benchmark/LatencyBenchmark/LatencyBenchmark.csproj -o out/LatencyBenchmark
      - name: Copy unified runner script
        run: cp unified-benchmark-runner.sh out/
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: BenchmarkRunner
          path: out/
          retention-days: 1

  build-and-push-image:
    if: needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch'
    needs: publish
    runs-on: ephemeral-env-runner
    strategy:
      matrix:
        servicename:
          [
            Aevatar.Silo,
            Aevatar.Developer.Host,
            Aevatar.HttpApi.Host,
            Aevatar.AuthServer,
            Aevatar.DbMigrator,
          ]
    permissions:
      contents: read
    outputs:
      short_sha: ${{ steps.vars.outputs.short_sha }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set short git commit SHA
        id: vars
        run: |
          calculatedSha=$(git rev-parse --short ${{ github.sha }})
          echo "short_sha=$calculatedSha" >> "$GITHUB_OUTPUT"
      - name: Download a single artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ matrix.servicename }}
          path: out/${{ matrix.servicename }}
      - name: Create image tag
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ secrets.REPOSITORY_REGION }}-docker.pkg.dev/${{ secrets.PROJECT_ID }}/${{ secrets.REPOSITORY }}/${{ matrix.servicename }}
          tags: |
            type=sha
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          build-args: |
            servicename=${{ matrix.servicename }}
            ENABLE_EPHEMERAL_CONFIG=true
          platforms: linux/amd64
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

  build-unified-benchmark-image:
    if: needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch'
    needs: [approval-count, publish-benchmarks]
    runs-on: ephemeral-env-runner
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Create image tag for unified benchmark
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ secrets.REPOSITORY_REGION }}-docker.pkg.dev/${{ secrets.PROJECT_ID }}/${{ secrets.REPOSITORY }}/benchmark-unified
          tags: |
            type=sha

      - name: Download benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          name: BenchmarkRunner
          path: out/

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push unified benchmark image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: Dockerfile.benchmark
          push: true
          platforms: linux/amd64
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

  setup-ephermeral-test-env:
    if: needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: ephemeral-env-runner
    needs: [build-and-push-image, build-unified-benchmark-image]
    outputs:
      auth_server_url: ${{ steps.extract.outputs.auth_server_url }}
      api_server_url: ${{ steps.extract.outputs.api_server_url }} 
      app_url: ${{ steps.extract.outputs.app_url }}
      client_id: ${{ steps.generate-uuid.outputs.client_id }}
      client_secret: ${{ steps.generate-uuid.outputs.client_secret }}
      project_name: ${{ steps.generate-uuid.outputs.project_name }}
    steps:
      - name: Generate UUID for project and client
        id: generate-uuid
        run: |
          UUID=$(python3 -c 'import uuid; print(str(uuid.uuid4())[:8])')
          echo "project_name=env-${UUID}" >> "$GITHUB_OUTPUT"
          echo "client_id=${UUID}" >> "$GITHUB_OUTPUT"
          echo "client_secret=${UUID}" >> "$GITHUB_OUTPUT"
      
      - name: Install GitHub CLI
        run: |
          curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
          sudo apt update
          sudo apt install gh -y
      
      - name: Setup Ephermeral Env
        id: setup_env
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.TOK }}
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: 'AElfDevops',
              repo: 'ephemeral-test-env',
              workflow_id: 'ephemeral-env-main.yaml',
              ref: 'main',
              inputs: {
                project_name: "${{ steps.generate-uuid.outputs.project_name }}",
                environment_ttl: "40",
                client_id: "${{ steps.generate-uuid.outputs.client_id }}",
                client_secret: "${{ steps.generate-uuid.outputs.client_secret }}",
                station_image_tag: "sha-${{ needs.build-and-push-image.outputs.short_sha }}"
              }
            });
            
      - name: Wait for workflow run to complete
        id: setup_wait
        env:
          GH_TOKEN: ${{ secrets.TOK }}
        run: |
          owner="AElfDevops"
          repo="ephemeral-test-env"
          workflow_id="ephemeral-env-main.yaml"

          echo "Waiting for workflow $workflow_id to start..."
          run_id=""

          # Wait for 30 seconds before attempting to get the workflow run
          echo "Sleeping for 30 seconds before checking workflow run..."
          sleep 30

          # Get the workflow run ID based on the latest triggered run
          run_id=$(gh api repos/$owner/$repo/actions/workflows/$workflow_id/runs \
            --jq '.workflow_runs | sort_by(.created_at) | reverse | .[0].id')

          # Wait until the workflow run appears
          for i in {1..30}; do
            run_id=$(gh api repos/$owner/$repo/actions/workflows/$workflow_id/runs \
              --jq '.workflow_runs[0].id')
            if [ -n "$run_id" ]; then
              echo "Run ID: $run_id"
              break
            fi
            echo "Workflow not yet started, retrying in 10s..."
            sleep 10
          done

          # Poll the run status
          echo "Waiting for workflow run to complete..."
          for i in {1..360}; do
            status=$(gh api repos/$owner/$repo/actions/runs/$run_id --jq '.status')
            conclusion=$(gh api repos/$owner/$repo/actions/runs/$run_id --jq '.conclusion')

            echo "Status: $status | Conclusion: $conclusion"
            if [ "$status" == "completed" ]; then
              if [ "$conclusion" != "success" ]; then
                echo "Workflow failed or was cancelled"
                exit 1
              fi
              break
            fi
            sleep 10
          done

          echo "Workflow completed successfully"

      - name: Download artifact from Devops repo
        uses: dawidd6/action-download-artifact@v2
        with:
          workflow: ephemeral-env-main.yaml
          repo: AElfDevops/ephemeral-test-env
          name: setup-aevatar-app-env-output  
          github_token: ${{ secrets.TOK }}
          path: ./downloaded

      - name: Extract env data from JSON
        id: extract
        run: |
          content=$(cat ./downloaded/setup-aevatar-app-env-output.json)
          echo "auth_server_url=$(echo $content | jq -r '.auth_server_url')" >> $GITHUB_OUTPUT
          echo "api_server_url=$(echo $content | jq -r '.api_server_url')" >> $GITHUB_OUTPUT
          echo "app_url=$(echo $content | jq -r '.app_url')" >> $GITHUB_OUTPUT

      - name: Set job summary
        run: |
          echo "## Aevatar App Created Successfully" >> $GITHUB_STEP_SUMMARY
          echo "- **Client ID**: \`${{ steps.generate-uuid.outputs.client_id }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Auth Server URL**: \`${{ steps.extract.outputs.auth_server_url }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **API Server URL**: \`${{ steps.extract.outputs.api_server_url }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **APP URL**: \`${{ steps.extract.outputs.app_url }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Configuration Mode**: Ephemeral (ENABLE_EPHEMERAL_CONFIG=true)" >> $GITHUB_STEP_SUMMARY

  run-regression-test:
    if: needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch'
    needs: setup-ephermeral-test-env 
    runs-on: ephemeral-env-runner
    env:
      AUTH_HOST: ${{ needs.setup-ephermeral-test-env.outputs.auth_server_url }}
      API_SERVER_HOST: ${{ needs.setup-ephermeral-test-env.outputs.api_server_url }}
      API_HOST: ${{ needs.setup-ephermeral-test-env.outputs.app_url }}
      CLIENT_ID: ${{ needs.setup-ephermeral-test-env.outputs.client_id }}
      CLIENT_SECRET: ${{ needs.setup-ephermeral-test-env.outputs.client_secret }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.arch }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.arch }}-pip-

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Python Script
        run: |
          source venv/bin/activate
          pytest -s -v station/scripts/regression_test.py
          
      - name: Lark Notification on Success
        if: success()
        uses: drayeasy/action-lark-notify@main
        env:
          LARK_WEBHOOK: ${{ secrets.LARK_WEBHOOK }}
          LARK_MESSAGE_TITLE: "Regression Testing Successful"

      - name: Lark Notification on Failure
        if: failure()
        uses: drayeasy/action-lark-notify@main
        env:
          LARK_WEBHOOK: ${{ secrets.LARK_WEBHOOK }}
          LARK_MESSAGE_TITLE: "Regression Test Failed"
          LARK_MESSAGE_TEMPLATE: "red"

  run-broadcast-benchmark:
    if: needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch'
    needs: [setup-ephermeral-test-env, run-regression-test, build-unified-benchmark-image]
    runs-on: ephemeral-env-runner
    timeout-minutes: 20
    permissions:
      contents: read
    outputs:
      job_name: ${{ steps.run_benchmark_job.outputs.job_name }}
      namespace: ${{ steps.run_benchmark_job.outputs.namespace }}
      status: ${{ steps.collect_results.outputs.status }}
    env:
      AUTH_HOST: ${{ needs.setup-ephermeral-test-env.outputs.auth_server_url }}
      API_SERVER_HOST: ${{ needs.setup-ephermeral-test-env.outputs.api_server_url }}
      API_HOST: ${{ needs.setup-ephermeral-test-env.outputs.app_url }}
      CLIENT_ID: ${{ needs.setup-ephermeral-test-env.outputs.client_id }}
      CLIENT_SECRET: ${{ needs.setup-ephermeral-test-env.outputs.client_secret }}
      PROJECT_NAME: ${{ needs.setup-ephermeral-test-env.outputs.project_name }}
      # Broadcast benchmark parameters - single source of truth
      BCAST_EPS: "1"
      BCAST_SUBS: "120"
      BCAST_PUBS: "1"
      BCAST_DURATION: "300"
      BCAST_WARMUP: "5"

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set short git commit SHA
        id: vars
        run: |
          calculatedSha=$(git rev-parse --short ${{ github.sha }})
          echo "short_sha=$calculatedSha" >> "$GITHUB_OUTPUT"

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3

      - name: Configure Kubernetes credentials
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBE_CONFIG }}
          
      - name: Wait for Ephemeral Environment Services
        run: |
          echo "🔍 Checking ephemeral environment services readiness..."
          echo "API Host: $API_HOST"
          echo "Project: $PROJECT_NAME"
          
          # Wait for API service to be ready
          for i in {1..30}; do
            if curl -fsS "$API_HOST/health" >/dev/null 2>&1; then 
              echo "✅ API service is ready"
              break
            fi
            echo "⏳ Waiting for API service... (attempt $i/30)"
            sleep 10
          done
          
          # Verify API service is accessible
          curl -fsS "$API_HOST/health" >/dev/null || {
            echo "❌ API service not accessible at $API_HOST/health"
            exit 1
          }
          
          echo "✅ Ephemeral environment is ready for benchmarking"

      - name: Run Benchmark Job in Ephemeral Environment
        id: run_benchmark_job
        run: |
          set -e
          # Set benchmark image with proper tag
          BENCHMARK_IMAGE="${{ secrets.REPOSITORY_REGION }}-docker.pkg.dev/${{ secrets.PROJECT_ID }}/${{ secrets.REPOSITORY }}/benchmark-unified:sha-${{ steps.vars.outputs.short_sha }}"
          echo "🚀 Creating benchmark job in ephemeral Kubernetes environment..."
          
          # Generate proper namespace following ephemeral-test-env naming convention
          PROJECT_NAME_NO_SPACES="${PROJECT_NAME// /}"
          NS="${PROJECT_NAME_NO_SPACES}-ephemeral-test-1"
          JOB_NAME="benchmark-runner-$(date +%s)"
          
          echo "Namespace: $NS"
          echo "Job Name: $JOB_NAME"
          echo "Benchmark Image: $BENCHMARK_IMAGE"
          
          # Create Kubernetes Job manifest for Broadcast Benchmark (Unified Image)
          cat > broadcast-benchmark-job.yaml << EOF
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: $JOB_NAME
            namespace: $NS
            labels:
              benchmark-type: broadcast
          spec:
            ttlSecondsAfterFinished: 300
            template:
              spec:
                restartPolicy: Never
                containers:
                - name: unified-benchmark
                  image: $BENCHMARK_IMAGE
                  env:
                  # Unified benchmark type selector
                  - name: BENCHMARK_TYPE
                    value: "broadcast"
                  # Common connection parameters
                  - name: CLIENT_ID
                    value: "$CLIENT_ID"
                  - name: SILO_SERVICE_HOST
                    value: "aevatar-silo-service.$NS.svc.cluster.local"
                  - name: SILO_GATEWAY_PORT
                    value: "20001"
                  # Common benchmark parameters
                  - name: COMMON_DURATION
                    value: "$BCAST_DURATION"
                  - name: COMMON_WARMUP
                    value: "$BCAST_WARMUP"
                  # Broadcast-specific parameters  
                  - name: BCAST_EPS
                    value: "$BCAST_EPS"
                  # Common thresholds
                  - name: COMMON_P95_TARGET
                    value: "120"
                  # Broadcast benchmark parameters
                  - name: BCAST_SUBS
                    value: "$BCAST_SUBS"
                  - name: BCAST_PUBS
                    value: "$BCAST_PUBS"
                  # Broadcast thresholds (using common P95 target)
                  - name: BCAST_P95_MS
                    value: "120"
                  - name: BCAST_SUCCESS_RATE
                    value: "0.90"
                  resources:
                    requests:
                      memory: "256Mi"
                      cpu: "200m"
          EOF
          
          # Apply the job
          kubectl apply -f broadcast-benchmark-job.yaml
          
          echo "✅ Broadcast benchmark job created successfully"
          echo "job_name=$JOB_NAME" >> $GITHUB_OUTPUT
          echo "namespace=$NS" >> $GITHUB_OUTPUT

      - name: Wait for Benchmark Completion
        run: |
          set -e
          JOB_NAME="${{ steps.run_benchmark_job.outputs.job_name }}"
          NS="${{ steps.run_benchmark_job.outputs.namespace }}"
          
          echo "⏳ Waiting for benchmark job to complete..."
          echo "Job: $JOB_NAME in namespace: $NS"
          
          # Wait for job completion (up to 20 minutes)
          for i in {1..120}; do
            status=$(kubectl get job $JOB_NAME -n $NS -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
            failed=$(kubectl get job $JOB_NAME -n $NS -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "")
            
            if [ "$status" = "True" ]; then
              echo "✅ Benchmark job completed successfully"
              break
            elif [ "$failed" = "True" ]; then
              echo "❌ Benchmark job failed"
              kubectl logs job/$JOB_NAME -n $NS || echo "No logs available"
              exit 1
            else
              echo "   ⏱️  Job still running... (check $i/120)"
              sleep 10
            fi
          done
          
          # Check if we timed out
          if [ "$status" != "True" ]; then
            echo "⚠️  Benchmark job timed out"
            kubectl logs job/$JOB_NAME -n $NS || echo "No logs available"
            exit 1
          fi

      - name: Collect Broadcast Benchmark Results
        id: collect_results
        if: always()
        run: |
          JOB_NAME="${{ steps.run_benchmark_job.outputs.job_name }}"
          NS="${{ steps.run_benchmark_job.outputs.namespace }}"
          
          echo "📊 Collecting broadcast benchmark results from job: $JOB_NAME"
          
          # Get pod name
          POD_NAME=$(kubectl get pods -l job-name=$JOB_NAME -n $NS -o jsonpath='{.items[0].metadata.name}')
          
          if [ -n "$POD_NAME" ]; then
            echo "Pod: $POD_NAME"
            
            # Simplified: Only collect logs, skip complex file copying
            echo "📝 Collecting benchmark logs..."
            kubectl logs $POD_NAME -n $NS > broadcast-benchmark.log 2>&1 || echo "Failed to get logs"
            
            # Determine status based on log analysis (will be refined by metrics check later)
            if grep -q "❌.*TARGET NOT ACHIEVED\|FAILED\|Error:" broadcast-benchmark.log; then
              STATUS="FAILED"
            elif grep -q "✅.*PASSED\|SUCCESS\|completed successfully" broadcast-benchmark.log; then
              STATUS="PASSED" 
            else
              STATUS="UNKNOWN"
            fi
            echo "status=$STATUS" >> $GITHUB_OUTPUT
            
            echo "📊 Broadcast Benchmark Summary:"
            grep -E "(📊|✅|❌|🎯|Status|Success Rate|Latency)" broadcast-benchmark.log || echo "Analysis complete"
            
          else
            echo "❌ Could not find broadcast benchmark pod"
            echo "status=ERROR" >> $GITHUB_OUTPUT
          fi

      - name: Process and Display Broadcast Results
        if: always()
        run: |
          echo "## 📊 Broadcast Latency Benchmark Report (Ephemeral K8s)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** Ephemeral Kubernetes Environment" >> $GITHUB_STEP_SUMMARY
          echo "**Project:** \`$PROJECT_NAME\` | **Client ID:** \`$CLIENT_ID\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Extract and parse benchmark results from logs
          if [ -f "broadcast-benchmark.log" ]; then
            echo "📊 Parsing broadcast benchmark results from logs..."
            # Extract key metrics from logs using grep and awk  
            success_rate=$(grep -o "Success Rate: [0-9.]*%" broadcast-benchmark.log | tail -1 | grep -o "[0-9.]*" || echo "0")
            p95_latency=$(grep "P95 Latency: [0-9.]*ms" broadcast-benchmark.log | tail -1 | sed 's/.*: \([0-9.]*\)ms.*/\1/' || echo "0")
            avg_latency=$(grep -o "Avg Latency: [0-9.]*ms" broadcast-benchmark.log | tail -1 | grep -o "[0-9.]*" || echo "0")
            throughput=$(grep -o "Throughput: [0-9.]* events/sec" broadcast-benchmark.log | tail -1 | grep -o "[0-9.]*" || echo "0")
            
            # Use broadcast-specific parameters (from env vars - single source of truth)
            subscribers="$BCAST_SUBS"
            publishers="$BCAST_PUBS" 
            duration="$BCAST_DURATION"
            eps="$BCAST_EPS"
            warmup="$BCAST_WARMUP"
            
            # Set thresholds
            success_rate_target="90.0"
            p95_target="120"
            
            echo "### 📊 Broadcast Latency Benchmark" >> $GITHUB_STEP_SUMMARY
            echo "**Test Parameters:** Subscribers: ${subscribers}, Publishers: ${publishers}, EPS: ${eps}, Duration: ${duration}s, Warmup: ${warmup}s" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value | Target |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|--------|" >> $GITHUB_STEP_SUMMARY
            echo "| Success Rate | ${success_rate}% | ≥${success_rate_target}% |" >> $GITHUB_STEP_SUMMARY
            echo "| Throughput | ${throughput} events/sec | ≥90.0% of expected |" >> $GITHUB_STEP_SUMMARY
            echo "| Avg Latency | ${avg_latency}ms | (no target) |" >> $GITHUB_STEP_SUMMARY
            echo "| P95 Latency | ${p95_latency}ms | ≤${p95_target}ms |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Determine pass/fail status
            pass_status="✅ PASSED"
            fail_reasons=""
            
            if awk -v a="$success_rate" -v b="$success_rate_target" 'BEGIN{ if (a+0 < b+0) exit 0; else exit 1 }'; then
              pass_status="❌ FAILED"
              fail_reasons="$fail_reasons Success rate below target. "
            fi
            
            if awk -v a="$p95_latency" -v b="$p95_target" 'BEGIN{ if (a+0 > b+0) exit 0; else exit 1 }'; then
              pass_status="❌ FAILED"
              fail_reasons="$fail_reasons P95 latency above target. "
            fi
            
            echo "### 🎯 Overall Assessment" >> $GITHUB_STEP_SUMMARY
            echo "**Status:** $pass_status" >> $GITHUB_STEP_SUMMARY
            if [ "$fail_reasons" != "" ]; then
              echo "**Issues:** $fail_reasons" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Update status output for benchmark-summary (broadcast)
            if [ "$pass_status" = "✅ PASSED" ]; then
              echo "status=PASSED" >> $GITHUB_OUTPUT
            else
              echo "status=FAILED" >> $GITHUB_OUTPUT
            fi
          else
            echo "❌ **No benchmark results available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "status=ERROR" >> $GITHUB_OUTPUT
          fi
          
          # Add execution details
          echo "### 📋 Execution Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Job Name:** \`${{ steps.run_benchmark_job.outputs.job_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Namespace:** \`${{ steps.run_benchmark_job.outputs.namespace }}\`" >> $GITHUB_STEP_SUMMARY

      - name: Cleanup Broadcast Benchmark Job
        if: always()
        run: |
          JOB_NAME="${{ steps.run_benchmark_job.outputs.job_name }}"
          NS="${{ steps.run_benchmark_job.outputs.namespace }}"
          
          if [ -n "$JOB_NAME" ] && [ -n "$NS" ]; then
            echo "🧹 Cleaning up broadcast benchmark job: $JOB_NAME"
            kubectl delete job $JOB_NAME -n $NS --ignore-not-found=true
            echo "✅ Cleanup completed"
          fi

      - name: Upload broadcast benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: broadcast-benchmark-results
          path: |
            broadcast-results/
            broadcast-benchmark.log
            broadcast-benchmark-job.yaml
          retention-days: 7

  run-latency-benchmark:
    if: needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch'
    needs: [setup-ephermeral-test-env, run-broadcast-benchmark, build-unified-benchmark-image]
    runs-on: ephemeral-env-runner
    timeout-minutes: 20
    permissions:
      contents: read
    outputs:
      job_name: ${{ steps.run_benchmark_job.outputs.job_name }}
      namespace: ${{ steps.run_benchmark_job.outputs.namespace }}
      status: ${{ steps.collect_results.outputs.status }}
    env:
      AUTH_HOST: ${{ needs.setup-ephermeral-test-env.outputs.auth_server_url }}
      API_SERVER_HOST: ${{ needs.setup-ephermeral-test-env.outputs.api_server_url }}
      API_HOST: ${{ needs.setup-ephermeral-test-env.outputs.app_url }}
      CLIENT_ID: ${{ needs.setup-ephermeral-test-env.outputs.client_id }}
      CLIENT_SECRET: ${{ needs.setup-ephermeral-test-env.outputs.client_secret }}
      PROJECT_NAME: ${{ needs.setup-ephermeral-test-env.outputs.project_name }}
      BROADCAST_STATUS: ${{ needs.run-broadcast-benchmark.outputs.status }}
      # Latency benchmark parameters - single source of truth
      LAT_EPS: "10"
      LAT_MAX_CONCURRENCY: "16"
      LAT_DURATION: "120"
      LAT_WARMUP: "5"
      LAT_START_FROM_LEVEL: "16"
      LAT_STOP_AT_LEVEL: "16"

    steps:
      - name: Check Broadcast Benchmark Status
        run: |
          echo "🔍 Checking broadcast benchmark status: $BROADCAST_STATUS"
          if [ "$BROADCAST_STATUS" = "FAILED" ] || [ "$BROADCAST_STATUS" = "ERROR" ]; then
            echo "⚠️  Broadcast benchmark failed, but continuing with latency benchmark"
          else
            echo "✅ Broadcast benchmark completed successfully"
          fi

      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set short git commit SHA
        id: vars
        run: |
          calculatedSha=$(git rev-parse --short ${{ github.sha }})
          echo "short_sha=$calculatedSha" >> "$GITHUB_OUTPUT"

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3

      - name: Configure Kubernetes credentials
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBE_CONFIG }}

      - name: Wait for Ephemeral Environment Services
        run: |
          echo "🔍 Checking ephemeral environment services readiness..."
          echo "API Host: $API_HOST"
          
          # Wait for API service to be ready
          for i in {1..15}; do
            if curl -fsS "$API_HOST/health" >/dev/null 2>&1; then 
              echo "✅ API service is ready"
              break
            fi
            echo "⏳ Waiting for API service... (attempt $i/15)"
            sleep 5
          done
          
          # Verify API service is accessible
          curl -fsS "$API_HOST/health" >/dev/null || {
            echo "❌ API service not accessible at $API_HOST/health"
            exit 1
          }

      - name: Run Latency Benchmark Job in Ephemeral Environment
        id: run_benchmark_job
        run: |
          set -e
          # Set benchmark image with proper tag
          BENCHMARK_IMAGE="${{ secrets.REPOSITORY_REGION }}-docker.pkg.dev/${{ secrets.PROJECT_ID }}/${{ secrets.REPOSITORY }}/benchmark-unified:sha-${{ steps.vars.outputs.short_sha }}"
          echo "🚀 Creating latency benchmark job in ephemeral Kubernetes environment..."
          
          # Generate proper namespace following ephemeral-test-env naming convention
          PROJECT_NAME_NO_SPACES="${PROJECT_NAME// /}"
          NS="${PROJECT_NAME_NO_SPACES}-ephemeral-test-1"
          JOB_NAME="latency-benchmark-$(date +%s)"
          
          echo "Namespace: $NS"
          echo "Job Name: $JOB_NAME"
          echo "Benchmark Image: $BENCHMARK_IMAGE"
          
          # Create Kubernetes Job manifest for Latency Benchmark (Unified Image)
          cat > latency-benchmark-job.yaml << EOF
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: $JOB_NAME
            namespace: $NS
            labels:
              benchmark-type: latency
          spec:
            ttlSecondsAfterFinished: 300
            template:
              spec:
                restartPolicy: Never
                containers:
                - name: unified-benchmark
                  image: $BENCHMARK_IMAGE
                  env:
                  # Unified benchmark type selector
                  - name: BENCHMARK_TYPE
                    value: "latency"
                  # Common connection parameters
                  - name: CLIENT_ID
                    value: "$CLIENT_ID"
                  - name: SILO_SERVICE_HOST
                    value: "aevatar-silo-service.$NS.svc.cluster.local"
                  - name: SILO_GATEWAY_PORT
                    value: "20001"
                  # Common benchmark parameters  
                  - name: COMMON_DURATION
                    value: "$LAT_DURATION"
                  - name: COMMON_WARMUP
                    value: "$LAT_WARMUP"
                  # Latency-specific parameters
                  - name: LAT_EPS
                    value: "$LAT_EPS"
                  # Common thresholds
                  - name: COMMON_P95_TARGET
                    value: "120"
                  # Latency benchmark parameters
                  - name: LAT_MAX_CONCURRENCY
                    value: "$LAT_MAX_CONCURRENCY"
                  - name: LAT_START_FROM_LEVEL
                    value: "$LAT_START_FROM_LEVEL"
                  - name: LAT_STOP_AT_LEVEL
                    value: "$LAT_STOP_AT_LEVEL"
                  # Latency thresholds (using common P95 target) 
                  - name: LAT_P95_MS
                    value: "120"
                  - name: LAT_P99_MS
                    value: "1000"
                  - name: LAT_PROCESSED_RATIO
                    value: "1.0"
                  resources:
                    requests:
                      memory: "256Mi"
                      cpu: "200m"
                    limits:
                      memory: "512Mi"
                      cpu: "500m"
          EOF
          
          # Apply the job
          kubectl apply -f latency-benchmark-job.yaml
          
          echo "✅ Latency benchmark job created successfully"
          echo "job_name=$JOB_NAME" >> $GITHUB_OUTPUT
          echo "namespace=$NS" >> $GITHUB_OUTPUT

      - name: Wait for Latency Benchmark Completion
        run: |
          set -e
          JOB_NAME="${{ steps.run_benchmark_job.outputs.job_name }}"
          NS="${{ steps.run_benchmark_job.outputs.namespace }}"
          
          echo "⏳ Waiting for latency benchmark job to complete..."
          echo "Job: $JOB_NAME in namespace: $NS"
          
          # Wait for job completion (up to 15 minutes)
          for i in {1..90}; do
            status=$(kubectl get job $JOB_NAME -n $NS -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
            failed=$(kubectl get job $JOB_NAME -n $NS -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "")
            
            if [ "$status" = "True" ]; then
              echo "✅ Latency benchmark job completed successfully"
              break
            elif [ "$failed" = "True" ]; then
              echo "❌ Latency benchmark job failed"
              kubectl logs job/$JOB_NAME -n $NS || echo "No logs available"
              exit 1
            else
              echo "   ⏱️  Job still running... (check $i/90)"
              sleep 10
            fi
          done
          
          # Check if we timed out
          if [ "$status" != "True" ]; then
            echo "⚠️  Latency benchmark job timed out"
            kubectl logs job/$JOB_NAME -n $NS || echo "No logs available"
            exit 1
          fi

      - name: Collect Latency Benchmark Results
        id: collect_results
        if: always()
        run: |
          JOB_NAME="${{ steps.run_benchmark_job.outputs.job_name }}"
          NS="${{ steps.run_benchmark_job.outputs.namespace }}"
          
          echo "📊 Collecting latency benchmark results from job: $JOB_NAME"
          
          # Get pod name
          POD_NAME=$(kubectl get pods -l job-name=$JOB_NAME -n $NS -o jsonpath='{.items[0].metadata.name}')
          
          if [ -n "$POD_NAME" ]; then
            echo "Pod: $POD_NAME"
            
            # Simplified: Only collect logs, skip complex file copying
            echo "📝 Collecting benchmark logs..."
            kubectl logs $POD_NAME -n $NS > latency-benchmark.log 2>&1 || echo "Failed to get logs"
            
            # Determine status based on log analysis (will be refined by metrics check later)
            if grep -q "❌.*TARGET NOT ACHIEVED\|FAILED\|Error:" latency-benchmark.log; then
              STATUS="FAILED"
            elif grep -q "✅.*PASSED\|SUCCESS\|completed successfully" latency-benchmark.log; then
              STATUS="PASSED" 
            else
              STATUS="UNKNOWN"
            fi
            echo "status=$STATUS" >> $GITHUB_OUTPUT
            
            echo "📊 Latency Benchmark Summary:"
            grep -E "(📊|✅|❌|🎯|Status|Latency|Throughput)" latency-benchmark.log || echo "Analysis complete"
            
          else
            echo "❌ Could not find latency benchmark pod"
            echo "status=ERROR" >> $GITHUB_OUTPUT
          fi

      - name: Process and Display Latency Results
        if: always()
        run: |
          echo "## ⚡ Latency Benchmark Report (Ephemeral K8s)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** Ephemeral Kubernetes Environment" >> $GITHUB_STEP_SUMMARY
          echo "**Project:** \`$PROJECT_NAME\` | **Client ID:** \`$CLIENT_ID\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Extract and parse latency benchmark results from logs
          if [ -f "latency-benchmark.log" ]; then
            echo "📊 Parsing latency benchmark results from logs..."
            # Extract metrics from logs using various patterns
            p95_latency=$(grep "P95 Latency: [0-9.]*ms\|P95: [0-9.]*ms" latency-benchmark.log | tail -1 | sed 's/.*: \([0-9.]*\)ms.*/\1/' || echo "0")
            p99_latency=$(grep "P99 Latency: [0-9.]*ms\|P99: [0-9.]*ms" latency-benchmark.log | tail -1 | sed 's/.*: \([0-9.]*\)ms.*/\1/' || echo "0")
            throughput=$(grep -o "Throughput: [0-9.]* events/sec\|ActualThroughput.*[0-9.]*" latency-benchmark.log | tail -1 | grep -o "[0-9.]*" || echo "0")
            processed_ratio=$(grep -o "Processed Ratio: [0-9.]*%\|processed.*[0-9.]*%" latency-benchmark.log | tail -1 | grep -o "[0-9.]*" || echo "0")
            
            # Use latency-specific parameters (from env vars - single source of truth)
            max_concurrency="$LAT_MAX_CONCURRENCY"
            duration="$LAT_DURATION"
            eps="$LAT_EPS"
            warmup="$LAT_WARMUP"
            
            # Set thresholds
            p95_target="120"
            p99_target="1000"
            processed_ratio_target="100.0"
            
            echo "### ⚡ Latency Benchmark" >> $GITHUB_STEP_SUMMARY
            echo "**Test Parameters:** Max Concurrency: ${max_concurrency}, EPS: ${eps}, Duration: ${duration}s, Warmup: ${warmup}s" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value | Target |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|--------|" >> $GITHUB_STEP_SUMMARY
            echo "| Throughput | ${throughput} events/sec | ≥90.0% of expected |" >> $GITHUB_STEP_SUMMARY
            echo "| P95 Latency | ${p95_latency}ms | ≤${p95_target}ms |" >> $GITHUB_STEP_SUMMARY
            echo "| P99 Latency | ${p99_latency}ms | (no target, max: ${p99_target}ms) |" >> $GITHUB_STEP_SUMMARY
            echo "| Processed Ratio | ${processed_ratio}% | ≥${processed_ratio_target}% |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Determine overall pass/fail status based on metrics (not unreliable log parsing)
            pass_status="✅ PASSED"
            fail_reasons=""
            
            if awk -v a="$p95_latency" -v b="$p95_target" 'BEGIN{ if (a+0 > b+0) exit 0; else exit 1 }'; then
              pass_status="❌ FAILED"
              fail_reasons="$fail_reasons P95 latency above target. "
            fi
            
            if awk -v a="$p99_latency" -v b="$p99_target" 'BEGIN{ if (a+0 > b+0) exit 0; else exit 1 }'; then
              pass_status="❌ FAILED"
              fail_reasons="$fail_reasons P99 latency above limit. "
            fi
            
            if awk -v a="$processed_ratio" -v b="$processed_ratio_target" 'BEGIN{ if (a+0 < b+0) exit 0; else exit 1 }'; then
              pass_status="❌ FAILED"
              fail_reasons="$fail_reasons Processed ratio below target. "
            fi
            
            echo "### 🎯 Overall Assessment" >> $GITHUB_STEP_SUMMARY
            echo "**Status:** $pass_status" >> $GITHUB_STEP_SUMMARY
            if [ "$fail_reasons" != "" ]; then
              echo "**Issues:** $fail_reasons" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Update status output for benchmark-summary (latency)
            if [ "$pass_status" = "✅ PASSED" ]; then
              echo "status=PASSED" >> $GITHUB_OUTPUT
            else
              echo "status=FAILED" >> $GITHUB_OUTPUT
            fi
          else
            echo "❌ **No benchmark results available**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "status=ERROR" >> $GITHUB_OUTPUT
          fi
          
          # Add execution details
          echo "### 📋 Execution Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Job Name:** \`${{ steps.run_benchmark_job.outputs.job_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Namespace:** \`${{ steps.run_benchmark_job.outputs.namespace }}\`" >> $GITHUB_STEP_SUMMARY

      - name: Cleanup Latency Benchmark Job
        if: always()
        run: |
          JOB_NAME="${{ steps.run_benchmark_job.outputs.job_name }}"
          NS="${{ steps.run_benchmark_job.outputs.namespace }}"
          
          if [ -n "$JOB_NAME" ] && [ -n "$NS" ]; then
            echo "🧹 Cleaning up latency benchmark job: $JOB_NAME"
            kubectl delete job $JOB_NAME -n $NS --ignore-not-found=true
            echo "✅ Cleanup completed"
          fi

      - name: Upload latency benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: latency-benchmark-results
          path: |
            latency-results/
            latency-benchmark.log
            latency-benchmark-job.yaml
          retention-days: 7

  benchmark-summary:
    if: always() && (needs.approval-count.outputs.approved_enough == 'true' || github.event_name == 'workflow_dispatch')
    needs: [run-broadcast-benchmark, run-latency-benchmark]
    runs-on: ephemeral-env-runner
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Generate Combined Benchmark Report
        run: |
          echo "## 🚀 Complete Performance Benchmarks Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📊 Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmark | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Broadcast Latency | \`${{ needs.run-broadcast-benchmark.outputs.status }}\` | Job: \`${{ needs.run-broadcast-benchmark.outputs.job_name }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Latency | \`${{ needs.run-latency-benchmark.outputs.status }}\` | Job: \`${{ needs.run-latency-benchmark.outputs.job_name }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Determine overall status
          BROADCAST_STATUS="${{ needs.run-broadcast-benchmark.outputs.status }}"
          LATENCY_STATUS="${{ needs.run-latency-benchmark.outputs.status }}"
          
          if [ "$BROADCAST_STATUS" = "PASSED" ] && [ "$LATENCY_STATUS" = "PASSED" ]; then
            echo "### ✅ Overall Result: ALL BENCHMARKS PASSED" >> $GITHUB_STEP_SUMMARY
            echo "✅ All benchmarks passed successfully"
          elif [ "$BROADCAST_STATUS" = "FAILED" ] || [ "$LATENCY_STATUS" = "FAILED" ]; then
            echo "### ⚠️  Overall Result: SOME BENCHMARKS FAILED" >> $GITHUB_STEP_SUMMARY
            echo "❌ Benchmark failure detected:"
            if [ "$BROADCAST_STATUS" = "FAILED" ]; then
              echo "  - Broadcast Latency: FAILED"
            fi
            if [ "$LATENCY_STATUS" = "FAILED" ]; then
              echo "  - Latency: FAILED"
            fi
            exit 1
          else
            echo "### ❌ Overall Result: BENCHMARK ERRORS OCCURRED" >> $GITHUB_STEP_SUMMARY
            echo "❌ Benchmark errors occurred:"
            echo "  - Broadcast Status: $BROADCAST_STATUS"
            echo "  - Latency Status: $LATENCY_STATUS"
            exit 1
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Note:** Individual benchmark results and detailed logs are available in the respective job artifacts." >> $GITHUB_STEP_SUMMARY

      - name: Final Lark Notification
        if: always()
        uses: drayeasy/action-lark-notify@main
        env:
          LARK_WEBHOOK: ${{ secrets.LARK_WEBHOOK }}
          LARK_MESSAGE_TITLE: "Ephemeral K8s Benchmarks Summary - Broadcast: ${{ needs.run-broadcast-benchmark.outputs.status }}, Latency: ${{ needs.run-latency-benchmark.outputs.status }}"
          LARK_MESSAGE_TEMPLATE: ${{ (needs.run-broadcast-benchmark.outputs.status == 'PASSED' && needs.run-latency-benchmark.outputs.status == 'PASSED') && 'green' || ((needs.run-broadcast-benchmark.outputs.status == 'ERROR' || needs.run-broadcast-benchmark.outputs.status == 'UNKNOWN' || needs.run-latency-benchmark.outputs.status == 'ERROR' || needs.run-latency-benchmark.outputs.status == 'UNKNOWN') && 'red' || 'yellow') }}