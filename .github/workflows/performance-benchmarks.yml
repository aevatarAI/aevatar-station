name: PR Performance Benchmarks

on:
  push:
    branches:
      - 'feature/**'
    paths:
      - 'station/benchmark/**'
      - 'station/src/**'
      - '.github/workflows/**'
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths:
      - 'station/benchmark/**'
      - 'station/src/**'
      - '.github/workflows/**'
  workflow_dispatch:
    inputs:
      broadcast_args:
        description: JSON for BroadcastLatencyBenchmark params
        required: false
        default: '{"subscribers":30,"publishers":1,"eps":5,"duration":20,"warmup":5}'
      latency_args:
        description: JSON for LatencyBenchmark params
        required: false
        default: '{"maxConcurrency":8,"duration":30,"warmup":5,"eps":10}'
      thresholds:
        description: JSON thresholds for benchmarks
        required: false
        default: '{"broadcast":{"avgMs":80,"p95Ms":150,"successRate":0.95,"throughputFactor":0.9},"latency":{"p95Ms":120,"p99Ms":220,"processedRatio":0.98,"throughputFactor":0.9}}'

concurrency:
  group: perf-pr-${{ github.ref }}
  cancel-in-progress: false

jobs:
  benchmarks:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    # Skip draft PRs, but allow manual workflow_dispatch and other events
    if: ${{ github.event_name != 'pull_request' || github.event.pull_request.draft == false }}
    timeout-minutes: 30
    env:
      # Threshold defaults (overridden by workflow_dispatch parsing step)
      BCAST_AVG_MS: '400'
      BCAST_P95_MS: '2000'
      BCAST_SUCCESS_RATE: '0.95'
      BCAST_THROUGHPUT_FACTOR: '0.9'
      LAT_P95_MS: '800'
      LAT_P99_MS: '1200'
      LAT_PROCESSED_RATIO: '0.95'
      LAT_THROUGHPUT_FACTOR: '0.9'
      # Param defaults (overridden by workflow_dispatch parsing step)
      BCAST_SUBS: '30'
      BCAST_PUBS: '1'
      BCAST_EPS: '5'
      BCAST_DURATION: '20'
      BCAST_WARMUP: '5'
      LAT_MAX_CONCURRENCY: '8'
      LAT_DURATION: '30'
      LAT_WARMUP: '5'
      LAT_EPS: '10'

    services:
      mongodb:
        image: mongo:7
        ports:
          - 27017:27017
        options: >-
          --health-cmd "mongosh --eval 'db.adminCommand(\"ping\")'"
          --health-interval 10s --health-timeout 5s --health-retries 10
      zookeeper:
        image: bitnami/zookeeper:3.9
        ports:
          - 2181:2181
        env:
          ALLOW_ANONYMOUS_LOGIN: 'yes'
      kafka:
        image: bitnami/kafka:3.7
        ports:
          - 9092:9092
        env:
          KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
          ALLOW_PLAINTEXT_LISTENER: 'yes'
          KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: 'true'
          KAFKA_LISTENERS: PLAINTEXT://:9092
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET 9 SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '9.0.x'

      - name: Show dotnet info
        run: dotnet --info

      - name: Install jq, curl, netcat
        run: sudo apt-get update && sudo apt-get install -y jq curl netcat-openbsd

      - name: Parse workflow_dispatch JSON inputs (if provided)
        if: ${{ github.event_name == 'workflow_dispatch' }}
        shell: bash
        run: |
          echo "broadcast_args=${{ toJson(inputs.broadcast_args) }}"
          echo "latency_args=${{ toJson(inputs.latency_args) }}"
          echo "thresholds=${{ toJson(inputs.thresholds) }}"

          # Guard empty inputs
          B_JSON='${{ inputs.broadcast_args }}'
          L_JSON='${{ inputs.latency_args }}'
          T_JSON='${{ inputs.thresholds }}'

          if [ -n "$B_JSON" ]; then
            export BCAST_SUBS=$(echo "$B_JSON" | jq -r '.subscribers // empty') || true
            export BCAST_PUBS=$(echo "$B_JSON" | jq -r '.publishers // empty') || true
            export BCAST_EPS=$(echo "$B_JSON" | jq -r '.eps // empty') || true
            export BCAST_DURATION=$(echo "$B_JSON" | jq -r '.duration // empty') || true
            export BCAST_WARMUP=$(echo "$B_JSON" | jq -r '.warmup // empty') || true
          fi

          if [ -n "$L_JSON" ]; then
            export LAT_MAX_CONCURRENCY=$(echo "$L_JSON" | jq -r '.maxConcurrency // empty') || true
            export LAT_DURATION=$(echo "$L_JSON" | jq -r '.duration // empty') || true
            export LAT_WARMUP=$(echo "$L_JSON" | jq -r '.warmup // empty') || true
            export LAT_EPS=$(echo "$L_JSON" | jq -r '.eps // empty') || true
          fi

          if [ -n "$T_JSON" ]; then
            export BCAST_AVG_MS=$(echo "$T_JSON" | jq -r '.broadcast.avgMs // empty') || true
            export BCAST_P95_MS=$(echo "$T_JSON" | jq -r '.broadcast.p95Ms // empty') || true
            export BCAST_SUCCESS_RATE=$(echo "$T_JSON" | jq -r '.broadcast.successRate // empty') || true
            export BCAST_THROUGHPUT_FACTOR=$(echo "$T_JSON" | jq -r '.broadcast.throughputFactor // empty') || true
            export LAT_P95_MS=$(echo "$T_JSON" | jq -r '.latency.p95Ms // empty') || true
            export LAT_P99_MS=$(echo "$T_JSON" | jq -r '.latency.p99Ms // empty') || true
            export LAT_PROCESSED_RATIO=$(echo "$T_JSON" | jq -r '.latency.processedRatio // empty') || true
            export LAT_THROUGHPUT_FACTOR=$(echo "$T_JSON" | jq -r '.latency.throughputFactor // empty') || true
          fi

          # Persist to GITHUB_ENV if set
          for v in BCAST_SUBS BCAST_PUBS BCAST_EPS BCAST_DURATION BCAST_WARMUP LAT_MAX_CONCURRENCY LAT_DURATION LAT_WARMUP LAT_EPS \
                   BCAST_AVG_MS BCAST_P95_MS BCAST_SUCCESS_RATE BCAST_THROUGHPUT_FACTOR LAT_P95_MS LAT_P99_MS LAT_PROCESSED_RATIO LAT_THROUGHPUT_FACTOR; do
            if [ -n "${!v}" ] && [ "${!v}" != "null" ]; then echo "$v=${!v}" >> $GITHUB_ENV; fi
          done

      - name: Wait for MongoDB (TCP)
        shell: bash
        run: |
          for i in {1..60}; do
            if nc -z localhost 27017; then echo "MongoDB port open"; break; fi; sleep 2; done
          nc -z localhost 27017 || (echo "MongoDB not reachable" && exit 1)

      - name: Wait for Kafka (simple socket check)
        shell: bash
        run: |
          for i in {1..120}; do
            if nc -z localhost 9092; then echo "Kafka port open"; break; fi; sleep 2; done

      - name: Restore and build projects
        run: |
          dotnet restore station/src/Aevatar.Silo/Aevatar.Silo.csproj
          dotnet restore station/benchmark/BroadcastLatencyBenchmark/BroadcastLatencyBenchmark.csproj
          dotnet restore station/benchmark/LatencyBenchmark/LatencyBenchmark.csproj
          dotnet build -c Release /p:TreatWarningsAsErrors=false station/src/Aevatar.Silo/Aevatar.Silo.csproj
          dotnet build -c Release /p:TreatWarningsAsErrors=false station/benchmark/BroadcastLatencyBenchmark/BroadcastLatencyBenchmark.csproj
          dotnet build -c Release /p:TreatWarningsAsErrors=false station/benchmark/LatencyBenchmark/LatencyBenchmark.csproj

      - name: Start Scheduler Silo
        shell: bash
        run: |
          export UseEnvironmentVariables=true
          export AevatarOrleans__AdvertisedIP=127.0.0.1
          export AevatarOrleans__SiloPort=10001
          export AevatarOrleans__GatewayPort=20001
          export AevatarOrleans__SILO_NAME_PATTERN=Scheduler
          export HealthCheck__Port=18081
          # Orleans Dashboard explicit binding to avoid null/parse/port conflicts
          export AevatarOrleans__DashboardIp=127.0.0.1
          export AevatarOrleans__DashboardPort=19080
          # Use MongoDB event sourcing and point ES client to same Mongo at 27017
          export OrleansEventSourcing__Provider=mongodb
          export Orleans__MongoDBESClient=mongodb://localhost:27017
          nohup dotnet run -c Release --project station/src/Aevatar.Silo/Aevatar.Silo.csproj > silo-scheduler.log 2>&1 &
          echo $! > silo-scheduler.pid

      - name: Start User Silo
        shell: bash
        run: |
          export UseEnvironmentVariables=true
          export AevatarOrleans__AdvertisedIP=127.0.0.1
          export AevatarOrleans__SiloPort=10002
          export AevatarOrleans__GatewayPort=20002
          export AevatarOrleans__SILO_NAME_PATTERN=User
          export HealthCheck__Port=18082
          # Orleans Dashboard explicit binding to avoid null/parse/port conflicts
          export AevatarOrleans__DashboardIp=127.0.0.1
          export AevatarOrleans__DashboardPort=19081
          # Use MongoDB event sourcing and point ES client to same Mongo at 27017
          export OrleansEventSourcing__Provider=mongodb
          export Orleans__MongoDBESClient=mongodb://localhost:27017
          nohup dotnet run -c Release --project station/src/Aevatar.Silo/Aevatar.Silo.csproj > silo-user.log 2>&1 &
          echo $! > silo-user.pid

      - name: Wait for Silo readiness (both)
        shell: bash
        run: |
          for i in {1..90}; do
            ok1=0; ok2=0
            curl -fsS http://localhost:18081/health/ready >/dev/null && ok1=1 || true
            curl -fsS http://localhost:18082/health/ready >/dev/null && ok2=1 || true
            if [ "$ok1" -eq 1 ] && [ "$ok2" -eq 1 ]; then echo "Both silos ready"; break; fi;
            if [ -f silo-scheduler.log ]; then tail -n 10 silo-scheduler.log || true; fi
            if [ -f silo-user.log ]; then tail -n 10 silo-user.log || true; fi
            sleep 2;
          done
          curl -fsS http://localhost:18081/health/ready >/dev/null || (echo "Scheduler silo not ready" && tail -n 200 silo-scheduler.log || true && exit 1)
          curl -fsS http://localhost:18082/health/ready >/dev/null || (echo "User silo not ready" && tail -n 200 silo-user.log || true && exit 1)

      - name: Run BroadcastLatencyBenchmark
        if: ${{ success() }}
        shell: bash
        run: |
          set -e
          dotnet run -c Release -p:TreatWarningsAsErrors=false --project station/benchmark/BroadcastLatencyBenchmark/BroadcastLatencyBenchmark.csproj -- \
            --subscriber-count ${BCAST_SUBS} \
            --publisher-count ${BCAST_PUBS} \
            --events-per-second ${BCAST_EPS} \
            --duration ${BCAST_DURATION} \
            --warmup-duration ${BCAST_WARMUP} \
            --output-file broadcast-latency-results.json

      - name: Run LatencyBenchmark
        if: ${{ success() }}
        shell: bash
        run: |
          set -e
          dotnet run -c Release --project station/benchmark/LatencyBenchmark/LatencyBenchmark.csproj -- \
            --max-concurrency ${LAT_MAX_CONCURRENCY} \
            --events-per-second ${LAT_EPS} \
            --duration ${LAT_DURATION} \
            --warmup-duration ${LAT_WARMUP} \
            --output-file latency-results.json

      - name: Parse and check thresholds - Broadcast
        id: check_broadcast
        if: ${{ success() }}
        shell: bash
        run: |
          set -e
          test -f broadcast-latency-results.json || (echo "Missing broadcast-latency-results.json" && exit 1)
          jq '.' broadcast-latency-results.json > /dev/null || (echo "Invalid broadcast JSON" && exit 1)
          jq -e '.Results | length > 0' broadcast-latency-results.json > /dev/null || (echo "Broadcast results are empty" && exit 1)
          jq -r '
            .Results[0] as $r
            | (($r.TotalEventsSent * ($r.SubscriberCount // 0))) as $expected
            | (if ($r.ActualDurationSeconds // 0) > 0 then ($r.TotalEventsProcessed / $r.ActualDurationSeconds) else 0 end) as $throughput
            | (if $expected > 0 then ($r.TotalEventsProcessed / $expected) else 0 end) as $successRate
            | (($r.PublisherCount // 0) * ($r.EventsPerSecond // 0)) as $expectedTp
            | {
                successRate: $successRate,
                throughput: $throughput,
                avg: ($r.AverageLatencyMs // 0),
                p95: ($r.P95LatencyMs // 0),
                expectedThroughput: $expectedTp
              }' broadcast-latency-results.json > broadcast_metrics.json

          sr=$(jq -r '.successRate' broadcast_metrics.json)
          tp=$(jq -r '.throughput' broadcast_metrics.json)
          avg=$(jq -r '.avg' broadcast_metrics.json)
          p95=$(jq -r '.p95' broadcast_metrics.json)
          etp=$(jq -r '.expectedThroughput' broadcast_metrics.json)

          # Format and display broadcast results
          echo "📊 Broadcast Latency Benchmark Results:"
          printf "   ✅ Success Rate: %.1f%% (target: ≥%.1f%%)\n" $(echo "$sr * 100" | bc -l) $(echo "$BCAST_SUCCESS_RATE * 100" | bc -l)
          printf "   ⚡ Throughput: %.1f events/sec" $tp
          if [ "${etp}" != "null" ] && [ "${etp}" != "0" ]; then
            printf " (target: ≥%.1f events/sec)" $(echo "$etp * $BCAST_THROUGHPUT_FACTOR" | bc -l)
          fi
          echo ""
          printf "   ⏱️  Avg Latency: %.1fms (target: ≤%sms)\n" $avg $BCAST_AVG_MS
          printf "   📈 P95 Latency: %.1fms (target: ≤%sms)\n" $p95 $BCAST_P95_MS
          echo ""

          # Store threshold check results but don't exit yet
          fail=0
          awk -v a="$sr" -v b="$BCAST_SUCCESS_RATE" 'BEGIN{ if (a+0 < b+0) exit 1 }' || { echo "❌ Broadcast successRate below $BCAST_SUCCESS_RATE"; fail=1; }
          if [ "${etp}" != "null" ] && [ "${etp}" != "0" ]; then
            awk -v a="$tp" -v b="$etp" -v f="$BCAST_THROUGHPUT_FACTOR" 'BEGIN{ if (a+0 < b*f) exit 1 }' || { echo "❌ Broadcast throughput below factor $BCAST_THROUGHPUT_FACTOR of expected"; fail=1; }
          fi
          awk -v a="$avg" -v b="$BCAST_AVG_MS" 'BEGIN{ if (a+0 > b+0) exit 1 }' || { echo "❌ Broadcast avg latency above $BCAST_AVG_MS ms"; fail=1; }
          awk -v a="$p95" -v b="$BCAST_P95_MS" 'BEGIN{ if (a+0 > b+0) exit 1 }' || { echo "❌ Broadcast P95 latency above $BCAST_P95_MS ms"; fail=1; }

          # Store the result for later use
          echo "$fail" > broadcast_threshold_result.txt
          
          if [ "$fail" -ne 0 ]; then
            echo "🚫 Broadcast benchmark failed threshold checks"
          else
            echo "✅ Broadcast benchmark passed all thresholds"
          fi

      - name: Parse and check thresholds - Latency
        id: check_latency
        if: ${{ success() }}
        shell: bash
        run: |
          set -e
          test -f latency-results.json || (echo "Missing latency-results.json" && exit 1)
          jq '.' latency-results.json > /dev/null || (echo "Invalid latency JSON" && exit 1)
          jq -e '.Results | length > 0' latency-results.json > /dev/null || (echo "Latency results are empty" && exit 1)
          jq -r '
            .Results | max_by(.ConcurrencyLevel) as $r
            | (($r.ConcurrencyLevel // 0) * ($r.EventsPerSecond // 0)) as $expectedTp
            | (if ($r.TotalEventsSent // 0) > 0 then ($r.TotalEventsProcessed / $r.TotalEventsSent) else 0 end) as $processedRatio
            | {
                success: ($r.Success // false),
                throughput: ($r.ActualThroughput // 0),
                p95: ($r.P95LatencyMs // 0),
                p99: ($r.P99LatencyMs // 0),
                processedRatio: $processedRatio,
                expectedThroughput: $expectedTp
              }' latency-results.json > latency_metrics.json

          succ=$(jq -r '.success' latency_metrics.json)
          tp=$(jq -r '.throughput' latency_metrics.json)
          p95=$(jq -r '.p95' latency_metrics.json)
          p99=$(jq -r '.p99' latency_metrics.json)
          pratio=$(jq -r '.processedRatio' latency_metrics.json)
          etp=$(jq -r '.expectedThroughput' latency_metrics.json)

          # Format and display latency results
          echo "📊 Latency Benchmark Results:"
          if [ "$succ" = "true" ]; then
            echo "   ✅ Execution Status: SUCCESS"
          else
            echo "   ❌ Execution Status: FAILED"
          fi
          printf "   ⚡ Throughput: %.1f events/sec" $tp
          if [ "${etp}" != "null" ] && [ "${etp}" != "0" ]; then
            printf " (target: ≥%.1f events/sec)" $(echo "$etp * $LAT_THROUGHPUT_FACTOR" | bc -l)
          fi
          echo ""
          printf "   📈 P95 Latency: %.1fms (target: ≤%sms)\n" $p95 $LAT_P95_MS
          printf "   📊 P99 Latency: %.1fms (target: ≤%sms)\n" $p99 $LAT_P99_MS
          printf "   🎯 Processed Ratio: %.1f%% (target: ≥%.1f%%)\n" $(echo "$pratio * 100" | bc -l) $(echo "$LAT_PROCESSED_RATIO * 100" | bc -l)
          echo ""

          # Store threshold check results but don't exit yet
          fail=0
          if [ "$succ" != "true" ]; then echo "❌ Latency result marked unsuccessful"; fail=1; fi
          if [ "${etp}" != "null" ] && [ "${etp}" != "0" ]; then
            awk -v a="$tp" -v b="$etp" -v f="$LAT_THROUGHPUT_FACTOR" 'BEGIN{ if (a+0 < b*f) exit 1 }' || { echo "❌ Latency throughput below factor $LAT_THROUGHPUT_FACTOR of expected"; fail=1; }
          fi
          awk -v a="$p95" -v b="$LAT_P95_MS" 'BEGIN{ if (a+0 > b+0) exit 1 }' || { echo "❌ Latency P95 above $LAT_P95_MS ms"; fail=1; }
          awk -v a="$p99" -v b="$LAT_P99_MS" 'BEGIN{ if (a+0 > b+0) exit 1 }' || { echo "❌ Latency P99 above $LAT_P99_MS ms"; fail=1; }
          awk -v a="$pratio" -v b="$LAT_PROCESSED_RATIO" 'BEGIN{ if (a+0 < b+0) exit 1 }' || { echo "❌ Latency processed ratio below $LAT_PROCESSED_RATIO"; fail=1; }

          # Store the result for later use
          echo "$fail" > latency_threshold_result.txt
          
          if [ "$fail" -ne 0 ]; then
            echo "🚫 Latency benchmark failed threshold checks"
          else
            echo "✅ Latency benchmark passed all thresholds"
          fi

      - name: Write Job Summary
        if: always()
        shell: bash
        run: |
          echo "## 🚀 Performance Benchmarks Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f broadcast_metrics.json ]; then
            sr=$(jq -r '.successRate' broadcast_metrics.json)
            tp=$(jq -r '.throughput' broadcast_metrics.json)
            avg=$(jq -r '.avg' broadcast_metrics.json)
            p95=$(jq -r '.p95' broadcast_metrics.json)
            
            echo "### 📊 Broadcast Latency Benchmark" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value | Target |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|--------|" >> $GITHUB_STEP_SUMMARY
            echo "| Success Rate | $(printf "%.1f%%" $(echo "$sr * 100" | bc -l)) | ≥$(echo "$BCAST_SUCCESS_RATE * 100" | bc -l)% |" >> $GITHUB_STEP_SUMMARY
            echo "| Throughput | $(printf "%.1f" $tp) events/sec | ≥$(echo "$BCAST_THROUGHPUT_FACTOR * 100" | bc -l)% of expected |" >> $GITHUB_STEP_SUMMARY
            echo "| Avg Latency | $(printf "%.1f" $avg)ms | ≤${BCAST_AVG_MS}ms |" >> $GITHUB_STEP_SUMMARY
            echo "| P95 Latency | $(printf "%.1f" $p95)ms | ≤${BCAST_P95_MS}ms |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f latency_metrics.json ]; then
            succ=$(jq -r '.success' latency_metrics.json)
            tp=$(jq -r '.throughput' latency_metrics.json)
            p95=$(jq -r '.p95' latency_metrics.json)
            p99=$(jq -r '.p99' latency_metrics.json)
            pratio=$(jq -r '.processedRatio' latency_metrics.json)
            
            echo "### ⚡ Latency Benchmark" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value | Target |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|--------|" >> $GITHUB_STEP_SUMMARY
            if [ "$succ" = "true" ]; then
              echo "| Status | ✅ SUCCESS | ✅ SUCCESS |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| Status | ❌ FAILED | ✅ SUCCESS |" >> $GITHUB_STEP_SUMMARY
            fi
            echo "| Throughput | $(printf "%.1f" $tp) events/sec | ≥$(echo "$LAT_THROUGHPUT_FACTOR * 100" | bc -l)% of expected |" >> $GITHUB_STEP_SUMMARY
            echo "| P95 Latency | $(printf "%.1f" $p95)ms | ≤${LAT_P95_MS}ms |" >> $GITHUB_STEP_SUMMARY
            echo "| P99 Latency | $(printf "%.1f" $p99)ms | ≤${LAT_P99_MS}ms |" >> $GITHUB_STEP_SUMMARY
            echo "| Processed Ratio | $(printf "%.1f%%" $(echo "$pratio * 100" | bc -l)) | ≥$(echo "$LAT_PROCESSED_RATIO * 100" | bc -l)% |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Final Threshold Check
        if: always()
        shell: bash
        run: |
          echo ""
          echo "🔍 Final Threshold Validation:"
          
          broadcast_fail=0
          latency_fail=0
          
          if [ -f broadcast_threshold_result.txt ]; then
            broadcast_fail=$(cat broadcast_threshold_result.txt)
          fi
          
          if [ -f latency_threshold_result.txt ]; then
            latency_fail=$(cat latency_threshold_result.txt)
          fi
          
          total_fail=$((broadcast_fail + latency_fail))
          
          if [ "$broadcast_fail" -eq 0 ]; then
            echo "✅ Broadcast benchmark: PASSED"
          else
            echo "❌ Broadcast benchmark: FAILED"
          fi
          
          if [ "$latency_fail" -eq 0 ]; then
            echo "✅ Latency benchmark: PASSED"
          else
            echo "❌ Latency benchmark: FAILED"
          fi
          
          echo ""
          if [ "$total_fail" -eq 0 ]; then
            echo "🎉 All benchmarks passed threshold validation!"
            exit 0
          else
            echo "🚫 One or more benchmarks failed threshold validation"
            echo "Check the detailed results above for specific failures"
            exit 1
          fi

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            broadcast-latency-results.json
            latency-results.json
            broadcast_metrics.json
            latency_metrics.json
            broadcast_threshold_result.txt
            latency_threshold_result.txt
            silo-scheduler.log
            silo-user.log

      - name: Stop Silo
        if: always()
        shell: bash
        run: |
          if [ -f silo-scheduler.pid ]; then kill $(cat silo-scheduler.pid) || true; fi
          if [ -f silo-user.pid ]; then kill $(cat silo-user.pid) || true; fi

